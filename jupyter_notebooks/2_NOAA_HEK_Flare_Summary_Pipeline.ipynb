{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0fe6b3-94ce-4ebc-bc0a-886c0b171a09",
   "metadata": {},
   "source": [
    "### Extract and Filter Solar Proton Event Data (2010 and above)\n",
    "\n",
    "This section loads a catalog of solar proton events from a NOAA SEP dataset stored in an AWS S3 bucket. It filters the data to include only events starting in the year 2010 or later, and extracts the **Begin Time** and **Flare Peak Time** columns. This cleaned dataset will be used to query HEK for matching flare metadata.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "```\n",
    "\n",
    "#### Define Output Parameters\n",
    "```python\n",
    "OUTPUT_JSON_DIR = \"hek_events\"\n",
    "CSV_OUTPUT_FILE = \"flare_summary_final/flare_summary_peaks.csv\"\n",
    "DURATION_MINUTES = 15  # Time window (in minutes) for HEK event queries\n",
    "```\n",
    "\n",
    "#### Load SEP Catalog from S3\n",
    "```python\n",
    "url = \"https://helioconverter-web-application.s3.amazonaws.com/sep_catalogs/2025-07-01T21-26-54_solar_proton_events.csv\"\n",
    "df = pd.read_csv(url, dtype=str)  # Load all fields as strings to preserve timestamp formatting\n",
    "```\n",
    "\n",
    "#### Filter Events to Include Only Year 2010 and Above\n",
    "```python\n",
    "df = df[df[\"Begin Time Yr M/D (UTC)\"] >= \"2010\"]\n",
    "```\n",
    "\n",
    "#### Extract Begin and Flare Peak Times\n",
    "```python\n",
    "df_filtered = df[[\"Begin Time Yr M/D (UTC)\", \"Flare Peak Time (UTC)\"]].copy()\n",
    "df_filtered[\"Flare Peak Time (UTC)\"] = df_filtered[\"Flare Peak Time (UTC)\"].fillna(\"\")\n",
    "```\n",
    "\n",
    "#### Save Cleaned Data to CSV\n",
    "```python\n",
    "df_filtered.to_csv(\"flare_summary_final/noaa_flare_peaks_2010_and_above.csv\", index=False)\n",
    "```\n",
    "\n",
    "This CSV now contains only the relevant timing data and will serve as input for HEK queries in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bbb470c-4c3f-4372-bd5e-16c67887c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "timestamp = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "# Load CSV without parsing dates\n",
    "url = f\"https://helioconverter-web-application.s3.amazonaws.com/sep_catalogs/{timestamp}_solar_proton_events.csv\"\n",
    "df = pd.read_csv(url, dtype=str)\n",
    "\n",
    "# Filter by year\n",
    "df = df[df[\"Begin Time Yr M/D (UTC)\"] >= \"2010\"]\n",
    "\n",
    "# Select relevant columns\n",
    "df_filtered = df[[\"Begin Time Yr M/D (UTC)\", \"Flare Peak Time (UTC)\"]].copy()\n",
    "df_filtered[\"Flare Peak Time (UTC)\"] = df_filtered[\"Flare Peak Time (UTC)\"].fillna(\"\")\n",
    "\n",
    "df_filtered.to_csv(\"flare_summary_final/noaa_flare_peaks_2010_and_above.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521a38f-ec0e-45a0-b35e-c588de141a71",
   "metadata": {},
   "source": [
    "### HEK Metadata Extraction Based on Flare Peaks\n",
    "\n",
    "This section uses the NOAA flare peak times to extract additional metadata from the Heliophysics Event Knowledgebase (HEK) via the Helioviewer API.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Convert Flare Peak Strings to Timestamps\n",
    "Only flare peak entries with non-empty values are converted into `datetime` format. This is required for time-based queries.\n",
    "\n",
    "#### 2. Create Output Folder\n",
    "An output directory is created (if it doesn't already exist) to store the HEK event data returned as `.json` files. This ensures we don’t re-download existing data.\n",
    "\n",
    "#### 3. Query the Helioviewer HEK API\n",
    "For each flare peak, the API is queried using a ±15-minute time window around the peak. This helps capture flares that may be recorded slightly before or after the listed peak time.\n",
    "\n",
    "#### 4. Save the Response Locally\n",
    "Each response is saved as a JSON file named with the flare timestamp, allowing you to inspect individual results later and avoid repeated downloads during debugging.\n",
    "\n",
    "#### 5. Parse the Returned Event Data\n",
    "The returned JSON is parsed to extract solar flare records (`type = FL`). Only flares are kept from the HEK results, even if other event types are present.\n",
    "\n",
    "#### 6. Extract Key Metadata Fields\n",
    "The script pulls out the most relevant physical and observational properties of each flare, including:\n",
    "- Start, peak, and end time\n",
    "- Bounding boxes and coordinates\n",
    "- Instrument and observatory metadata\n",
    "- Peak flux (if available)\n",
    "\n",
    "#### 7. Store the Full Flare Metadata to CSV\n",
    "All extracted HEK flare records are written to a CSV file. Time fields are placed at the beginning of the file for clarity, followed by other metadata fields.\n",
    "\n",
    "---\n",
    "\n",
    "This HEK metadata will later be useful for identifying flare positions, bounding boxes, and cross-matching with SDO/AIA image data for visualization or ML applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720db90c-93fa-4425-b4a2-14e8aea51644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "OUTPUT_JSON_DIR = \"hek_events\"\n",
    "CSV_OUTPUT_FILE = \"flare_summary_final/flare_hek_peaks_data.csv\"\n",
    "DURATION_MINUTES = 15\n",
    "\n",
    "# Convert flare peak times to datetime (skip blanks)\n",
    "flare_peaks_dt = [\n",
    "    datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    for t in df_filtered[\"Flare Peak Time (UTC)\"] if t\n",
    "]\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_hek_events(timestamp, filename, duration_minutes=15):\n",
    "    url = \"https://api.helioviewer.org/v2/events/\"\n",
    "    start_time_str = timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_time_str = (timestamp + timedelta(minutes=duration_minutes)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    response = requests.get(url, params={\"startTime\": start_time_str, \"endTime\": end_time_str, \"sources\": \"HEK\"})\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\" HEK data saved to {filename} for {start_time_str}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\" Failed to fetch HEK events for {start_time_str}\")\n",
    "        return []\n",
    "\n",
    "def parse_hek_events(timestamp, filename):\n",
    "    parsed_data = fetch_hek_events(timestamp, filename, duration_minutes=DURATION_MINUTES)\n",
    "    shared_fields = [\n",
    "        \"boundbox_c1ll\", \"boundbox_c1ur\", \"boundbox_c2ll\", \"boundbox_c2ur\",\n",
    "        \"event_starttime\", \"event_endtime\", \"hpc_coord\", \"hpc_bbox\", \"hpc_radius\",\n",
    "        \"hv_hpc_x_final\", \"hv_hpc_y_final\", \"hv_labels_formatted\", \"frm_name\",\n",
    "        \"obs_observatory\", \"obs_instrument\", \"obs_channelid\", \"frm_url\", \"event_type\"\n",
    "    ]\n",
    "    flare_fields = shared_fields + [\"fl_peakflux\", \"fl_peakfluxunit\", \"event_peaktime\"]\n",
    "    hek_flares = []\n",
    "\n",
    "    for record in parsed_data:\n",
    "        if \"groups\" in record:\n",
    "            for group in record.get(\"groups\", []):\n",
    "                for item in group.get(\"data\", []):\n",
    "                    if item.get(\"type\") == \"FL\":\n",
    "                        flare = {k: item.get(k, None) for k in flare_fields}\n",
    "                        hek_flares.append(flare)\n",
    "        elif record.get(\"type\") == \"FL\":\n",
    "            flare = {k: record.get(k, None) for k in flare_fields}\n",
    "            hek_flares.append(flare)\n",
    "\n",
    "    print(f\" Found {len(hek_flares)} flares in {filename}\")\n",
    "    return hek_flares\n",
    "\n",
    "# Process each flare\n",
    "all_flares = []\n",
    "for peak_time in flare_peaks_dt:\n",
    "    iso_str = peak_time.strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    json_file = os.path.join(OUTPUT_JSON_DIR, f\"hek_{iso_str}.json\")\n",
    "\n",
    "    if os.path.exists(json_file):\n",
    "        print(f\" Skipping {json_file} (already exists)\")\n",
    "        with open(json_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = fetch_hek_events(peak_time, json_file)\n",
    "\n",
    "    flares = parse_hek_events(peak_time, json_file)\n",
    "    all_flares.extend(flares)\n",
    "\n",
    "# Save to CSV\n",
    "if all_flares:\n",
    "    time_fields = [\"event_starttime\", \"event_peaktime\", \"event_endtime\"]\n",
    "    other_fields = [k for k in all_flares[0].keys() if k not in time_fields]\n",
    "    fieldnames = time_fields + other_fields\n",
    "\n",
    "    with open(CSV_OUTPUT_FILE, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_flares)\n",
    "\n",
    "    print(f\" Wrote flare metadata for {len(all_flares)} events to {CSV_OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\" No flare metadata extracted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
