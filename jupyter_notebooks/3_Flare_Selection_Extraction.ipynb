{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f219d0e-8644-4975-9d6e-aa0d58bb4a54",
   "metadata": {},
   "source": [
    "### Extracting Solar Flare Events from 2014\n",
    "\n",
    "This code filters a CSV file to include only solar flare events that occurred in the year 2014 and saves the result to a new file. It also ensures the output directory exists before writing the file.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Import Libraries**  \n",
    "   `pandas` is used for data manipulation, and `os` is used to create directories if they don't exist.\n",
    "\n",
    "2. **Ensure Output Directory Exists**  \n",
    "   Before saving the filtered file, the script checks if the directory exists and creates it if needed.\n",
    "\n",
    "3. **Load the CSV File**  \n",
    "   The input file is read with date parsing enabled for the `event_starttime` and `event_endtime` columns.\n",
    "\n",
    "4. **Filter by Year (2014)**  \n",
    "   The dataset is filtered to retain only rows where the flare event started in the year 2014.\n",
    "\n",
    "5. **Save the Filtered Data**  \n",
    "   The filtered DataFrame is saved to the specified output path without row indices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43efd1d-ea4b-443a-84f2-113369b4ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_2014_events(csv_path, output_path):\n",
    "    \n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the data and parse the dates\n",
    "    df = pd.read_csv(csv_path, parse_dates=['event_starttime', 'event_endtime'])\n",
    "\n",
    "    # Filter for all of 2014\n",
    "    events_2014 = df[df['event_starttime'].dt.year == 2014]\n",
    "\n",
    "    # Save to new CSV\n",
    "    events_2014.to_csv(output_path, index=False)\n",
    "    print(f\" Saved {len(events_2014)} events from 2014 to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "extract_2014_events(\n",
    "    \"flare_summary_final/flare_hek_peaks_data.csv\",\n",
    "    \"flare_summary_final/flare_selection/flare_hek_peaks_data_2014.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b062d-9595-4c78-a411-f5220ad26998",
   "metadata": {},
   "source": [
    "### Extracting and Processing the Strongest Daily Flares with JSOC-Compatible Recordsets\n",
    "\n",
    "This function identifies the strongest solar flare per day from a CSV dataset, filters by optional instrument criteria, computes observation windows with buffer times, and generates JSOC-compatible recordset strings for each event. It also ensures that output directories exist and prints the resulting recordsets to the screen.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Directory Creation**  \n",
    "   Ensures the target directory for the output file exists using `os.makedirs`.\n",
    "\n",
    "2. **Data Loading**  \n",
    "   Loads the input CSV and parses the `event_starttime` and `event_endtime` columns as datetime objects.\n",
    "\n",
    "3. **Peak Flux Cleaning**  \n",
    "   Converts `fl_peakflux` to numeric and removes invalid or missing values.\n",
    "\n",
    "4. **Flexible Filtering** *(Optional)*  \n",
    "   Allows user-defined filtering by:\n",
    "   - `obs_observatory` (e.g., \"SDO\")\n",
    "   - `obs_instrument` (e.g., \"AIA\")\n",
    "   - `obs_channelid` (e.g., \"131\", \"94\")\n",
    "\n",
    "5. **Daily Selection**  \n",
    "   Groups flares by date and selects the one with the highest peak flux for each day.\n",
    "\n",
    "6. **Buffer Times and JSOC-Compatible Windows**  \n",
    "   Adds pre-flare and post-flare buffer windows (default = 178 minutes) and rounds them to the nearest minute for compatibility with JSOC data queries.\n",
    "\n",
    "7. **Duration and Recordset String Construction**  \n",
    "Calculates duration for each event and builds a JSOC recordset string in the format:\n",
    "\n",
    "    `aia.lev1_euv_12s[START_TIME/DURATIONm][WAVELENGTH]`\n",
    "\n",
    "8. **Column Reordering**  \n",
    "Moves all key time columns to the front of the CSV for readability:\n",
    "\n",
    "    `jsoc_start_time, pre_flare_start_time, event_starttime, event_peaktime, event_endtime, post_flare_end_time, jsoc_end_time`\n",
    "\n",
    "\n",
    "\n",
    "9. **Output**  \n",
    "- Saves the result to CSV.\n",
    "- Prints each generated JSOC recordset to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a88d1-5f3e-4fc2-93c0-3fe0d9f86eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "def extract_strongest_flares_per_day(\n",
    "    csv_path,\n",
    "    output_path,\n",
    "    observatory=\"any\",\n",
    "    instrument=\"any\",\n",
    "    wavelengths=None,\n",
    "    buffer_minutes=178,\n",
    "    wavelength=\"131\"\n",
    "):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path, parse_dates=['event_starttime', 'event_endtime'])\n",
    "\n",
    "    # Ensure fl_peakflux is numeric\n",
    "    df['fl_peakflux'] = pd.to_numeric(df['fl_peakflux'], errors='coerce')\n",
    "    df = df.dropna(subset=['fl_peakflux'])\n",
    "\n",
    "    # Apply filters\n",
    "    if observatory.lower() != \"any\":\n",
    "        df = df[df['obs_observatory'] == observatory]\n",
    "    if instrument.lower() != \"any\":\n",
    "        df = df[df['obs_instrument'] == instrument]\n",
    "    if wavelengths:\n",
    "        df = df[df['obs_channelid'].astype(str).isin([str(w) for w in wavelengths])]\n",
    "\n",
    "    # Extract flare date\n",
    "    df['flare_date'] = df['event_starttime'].dt.date\n",
    "\n",
    "    # Strongest per day\n",
    "    strongest = df.loc[df.groupby('flare_date')['fl_peakflux'].idxmax()].copy()\n",
    "    strongest = strongest.drop(columns=['flare_date'])\n",
    "\n",
    "    # Compute JSOC-compatible buffer times\n",
    "    strongest['pre_flare_start_time'] = strongest['event_starttime'] - timedelta(minutes=buffer_minutes)\n",
    "    strongest['post_flare_end_time'] = strongest['event_endtime'] + timedelta(minutes=buffer_minutes)\n",
    "    strongest['jsoc_start_time'] = strongest['pre_flare_start_time'].dt.floor('min')\n",
    "    strongest['jsoc_end_time'] = strongest['post_flare_end_time'].dt.floor('min')\n",
    "\n",
    "    '''\n",
    "    # Calculate duration and recordsets\n",
    "    strongest['duration'] = strongest['jsoc_end_time'] - strongest['jsoc_start_time']\n",
    "    strongest['recordset'] = strongest.apply(lambda row: (\n",
    "        f\"aia.lev1_euv_12s[{row['jsoc_start_time'].strftime('%Y-%m-%dT%H:%M:%S')}/\"\n",
    "        f\"{int(row['duration'].total_seconds() // 60)}m][{wavelength}]\"\n",
    "    ), axis=1)\n",
    "    '''\n",
    "    # Calculate duration and recordsets\n",
    "    strongest['duration'] = strongest['jsoc_end_time'] - strongest['jsoc_start_time']\n",
    "    strongest['duration'] = strongest['duration'].dt.total_seconds() // 60  # <-- This updates the column to minutes\n",
    "    strongest['recordset'] = strongest.apply(lambda row: (\n",
    "        f\"aia.lev1_euv_12s[{row['jsoc_start_time'].strftime('%Y-%m-%dT%H:%M:%S')}/\"\n",
    "        f\"{int(row['duration'])}m][{wavelength}]\"\n",
    "    ), axis=1)\n",
    "\n",
    "\n",
    "    # Ensure proper ordering of time columns at the front\n",
    "    time_cols = [\n",
    "        \"jsoc_start_time\",\n",
    "        \"pre_flare_start_time\",\n",
    "        \"event_starttime\",\n",
    "        \"event_peaktime\" if \"event_peaktime\" in strongest.columns else None,\n",
    "        \"event_endtime\",\n",
    "        \"post_flare_end_time\",\n",
    "        \"jsoc_end_time\"\n",
    "    ]\n",
    "    time_cols = [col for col in time_cols if col]  # Remove None if peaktime is missing\n",
    "    other_cols = [col for col in strongest.columns if col not in time_cols]\n",
    "    reordered = time_cols + other_cols\n",
    "    strongest = strongest[reordered]\n",
    "\n",
    "    # Save to CSV\n",
    "    strongest.to_csv(output_path, index=False)\n",
    "    print(f\" Saved strongest flares per day with JSOC recordsets to: {output_path}\")\n",
    "\n",
    "    # Print recordsets\n",
    "    print(\"\\n JSOC Recordsets:\\n\")\n",
    "    for r in strongest['recordset']:\n",
    "        print(r)\n",
    "\n",
    "# Example usage\n",
    "extract_strongest_flares_per_day(\n",
    "    csv_path=\"flare_summary_final/flare_selection/flare_hek_peaks_data_2014.csv\",\n",
    "    output_path=\"flare_summary_final/flare_selection/strongest_flares_2014_SDO_AIA_131.csv\",\n",
    "    observatory=\"SDO\",\n",
    "    instrument=\"AIA\",\n",
    "    wavelengths=[\"131\"],\n",
    "    buffer_minutes=178,\n",
    "    wavelength=\"131\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
