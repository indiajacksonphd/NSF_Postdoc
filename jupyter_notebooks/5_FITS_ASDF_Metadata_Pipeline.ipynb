{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c77b60c4-0872-4cb0-a253-7671ceaa27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asdf\n",
    "import io\n",
    "import requests\n",
    "import argparse\n",
    "from datetime import datetime, timedelta\n",
    "from sunpy.net import Fido, attrs as a\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import sunpy.map\n",
    "from sunpy.visualization.colormaps import color_tables as ct\n",
    "import boto3\n",
    "from dateutil import parser as dtparser\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import asdf\n",
    "import time\n",
    "import yaml\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "# The amount of storage needed for the ASDF files required an external hard drive\n",
    "EXTERNAL_DRIVE_BASE = \"/Volumes/Elements/HelioData\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c8204-c23f-4b91-9e2e-2a240275de28",
   "metadata": {},
   "source": [
    "## Fetching Solar Event Metadata from Helioviewer (HEK and DONKI)\n",
    "\n",
    "This section describes how solar event metadata is programmatically retrieved from the Helioviewer API using time-stamped observations of solar activity.\n",
    "\n",
    "Two types of event catalogs are available through the Helioviewer system:\n",
    "\n",
    "- **HEK (Heliophysics Event Knowledgebase):** A well-established NASA archive that catalogs solar events such as flares, active regions, and coronal mass ejections, primarily annotated by expert scientists and automated pipelines.\n",
    "- **DONKI (Database Of Notifications, Knowledge, and Information):** A real-time event catalog produced by the NASA CCMC (Community Coordinated Modeling Center), which includes operational alerts and model-driven forecasts for solar activity.\n",
    "\n",
    "For this study, **only the HEK metadata is used** to label and describe the solar images and to generate bounding boxes for flare-related activity. This decision is based on HEK‚Äôs consistency, scientific reliability, and widespread usage in heliophysics.\n",
    "\n",
    "The **DONKI data is retained** and saved alongside the FITS and HEK-derived annotations for potential **future use**. This allows flexibility for:\n",
    "- Comparative analysis with model-based events,\n",
    "- Exploration of operational forecasting potential,\n",
    "- Reuse by other researchers interested in real-time alerts.\n",
    "\n",
    "Both datasets are queried using short time windows (default: 15 minutes) centered on each FITS image‚Äôs timestamp. Metadata is saved locally in JSON format for transparency, reproducibility, and long-term archival.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03bd5815-b48c-4214-aff9-a58b802b8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_donki_events(timestamp, filename, duration_minutes=15):\n",
    "    url = \"https://api.helioviewer.org/v2/events/\"\n",
    "\n",
    "    start_time_str = timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_time = timestamp + timedelta(minutes=duration_minutes)\n",
    "    end_time_str = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    params = {\n",
    "        \"startTime\": start_time_str,\n",
    "        \"endTime\": end_time_str,\n",
    "        \"sources\": \"DONKI\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"DONKI data saved to {filename} for {start_time_str}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to fetch HEK events for {start_time_str}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_hek_events(timestamp, filename, duration_minutes=15):\n",
    "    url = \"https://api.helioviewer.org/v2/events/\"\n",
    "\n",
    "    start_time_str = timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_time = timestamp + timedelta(minutes=duration_minutes)\n",
    "    end_time_str = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    params = {\n",
    "        \"startTime\": start_time_str,\n",
    "        \"endTime\": end_time_str,\n",
    "        \"sources\": \"HEK\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"HEK data saved to {filename} for {start_time_str}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to fetch HEK events for {start_time_str}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88e74083-b177-47a8-bc23-ca1076c789ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_local(local_path, bucket, s3_key):\n",
    "    \"\"\"Simulates S3 upload by saving to a local directory using the same structure.\"\"\"\n",
    "    # Convert bucket name into a base local directory\n",
    "    base_dir = os.path.join(\".\", bucket)\n",
    "    full_path = os.path.join(base_dir, s3_key)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "    # Copy the file to the new location\n",
    "    with open(local_path, 'rb') as src_file:\n",
    "        with open(full_path, 'wb') as dst_file:\n",
    "            dst_file.write(src_file.read())\n",
    "\n",
    "    print(f\"Saved locally to {full_path}\")\n",
    "\n",
    "\n",
    "def save_to_s3(local_path, bucket, s3_key):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    print(f\"Uploading to s3://{bucket}/{s3_key}...\")\n",
    "    s3.upload_file(local_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c56c78-ea7a-439d-ae48-eba1ef3375c9",
   "metadata": {},
   "source": [
    "## Adding ML-Compatible Labels to ASDF Files\n",
    "\n",
    "Each FITS image is paired with a machine learning label indicating whether it corresponds to a solar flare (1) or a non-flaring region (0). These labels are stored in a pre-generated CSV file located at:\n",
    "\n",
    "```\n",
    "data_selection/image_labels.csv\n",
    "```\n",
    "\n",
    "This file contains two columns: `filename` (which includes the full FITS filename) and `label` (either 0 or 1). During the ASDF build process, the function performs the following:\n",
    "\n",
    "- **Loads the CSV file into memory** and standardizes formatting by stripping extra spaces and converting all filenames to strings.\n",
    "- **Constructs a normalized filename** based on the current FITS file being processed. This ensures a direct match with entries in the `image_labels.csv` file.\n",
    "- **Searches for the label** corresponding to the FITS filename. If found, the label is inserted under the `meta` section of the ASDF tree using the key `image_label`.\n",
    "- If the label is **not found**, a warning is printed, and `image_label` is set to `null` (i.e., `None` in Python) in the ASDF structure.\n",
    "\n",
    "This design supports traceable and reproducible machine learning workflows, allowing the ASDF file to encapsulate not only observational and scientific metadata, but also a direct classification label for supervised training or evaluation.\n",
    "\n",
    "Labels are critical to ensure that downstream CNN models can learn the difference between flare and non-flare imagery without needing to perform additional external joins or lookups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6571a9f5-80be-48f5-a9de-3d5ba1e191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df = pd.read_csv(\"data_selection/image_labels.csv\")\n",
    "EXTERNAL_DRIVE_BASE = \"/Volumes/Elements/HelioData\"\n",
    "\n",
    "def build_and_save_asdf(fname, hdul, hek_data, donki_data, image_paths, bucket, base_key, rootPath):\n",
    "    tree = OrderedDict()\n",
    "    \n",
    "    \n",
    "    labels_df = pd.read_csv(\"data_selection/image_labels.csv\")\n",
    "    labels_df['filename'] = labels_df['filename'].astype(str).str.strip()\n",
    "\n",
    "    # normalized_fname = fname.replace(\".asdf\", \"\").replace(\".fits\", \"\").strip()\n",
    "    normalized_fname = f\"{fname}.fits\".strip() if not fname.endswith(\".fits\") else fname.strip()\n",
    "\n",
    "\n",
    "    label_row = labels_df[labels_df[\"filename\"] == normalized_fname]\n",
    "\n",
    "    if not label_row.empty:\n",
    "        image_label = int(label_row[\"label\"].values[0])\n",
    "    else:\n",
    "        print(f\"Label not found for: {normalized_fname}\")\n",
    "        image_label = None\n",
    "\n",
    "\n",
    "    tree[\"meta\"] = {\n",
    "        \"creator\": \"Dr. India R Jackson\",\n",
    "        \"institution\": \"Georgia State University\",\n",
    "        \"nsf_award\": \"AGS-PRF #2444918\",\n",
    "        \"description\": \"ASDF archive for SDO image, HEK, and DONKI data with ML-ready FITS structure.\",\n",
    "        \"pipeline_version\": \"1.0.0\",\n",
    "        \"script\": \"HelioConverter\",\n",
    "        \"pipeline_timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"image_label\": image_label\n",
    "    }\n",
    "\n",
    "    tree[\"fits\"] = {\n",
    "        \"source_file\": f\"s3://{bucket}/{base_key}/fits/raw/{fname}.fits\",\n",
    "        \"num_hdus\": len(hdul),\n",
    "        \"hdu0\": {\n",
    "            \"header\": full_header_to_dict(hdul[0].header),\n",
    "            \"data\": hdul[0].data.tolist() if hdul[0].data is not None else None\n",
    "        },\n",
    "        \"hdu1\": {\n",
    "            \"header\": full_header_to_dict(hdul[1].header) if len(hdul) > 1 else None,\n",
    "            \"data\": hdul[1].data.tolist() if len(hdul) > 1 and hdul[1].data is not None else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    tree[\"hek\"] = hek_data\n",
    "    # tree[\"donki\"] = donki_data\n",
    "    tree[\"images\"] = image_paths\n",
    "    \n",
    "    # asdf_path = os.path.join(rootPath, f\"{fname}.asdf\")\n",
    "    \n",
    "    # asdf_dir = os.path.join(\"final_fits_1\", \"asdf\")\n",
    "    asdf_dir = os.path.join(EXTERNAL_DRIVE_BASE, \"asdf\")\n",
    "    os.makedirs(asdf_dir, exist_ok=True)\n",
    "    asdf_path = os.path.join(asdf_dir, f\"{fname}.asdf\")\n",
    "\n",
    "    \n",
    "    af = asdf.AsdfFile(tree)\n",
    "    af.write_to(asdf_path, all_array_compression='zlib')\n",
    "    print(f\"ASDF saved locally: {asdf_path}\")\n",
    "    \n",
    "    '''\n",
    "    asdf_s3_key = f\"{base_key}/asdf/{fname}.asdf\"\n",
    "    save_to_s3(asdf_path, bucket, asdf_s3_key)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d09568-3636-4916-9eba-f7a84c59e17f",
   "metadata": {},
   "source": [
    "## Converting FITS Headers into Serializable Dictionaries\n",
    "\n",
    "The `full_header_to_dict` function is used to convert a FITS Header object into a fully serializable Python dictionary that can be saved in formats such as JSON or included in ASDF files. This is a critical step when preparing astronomical data for archival, sharing, or machine learning workflows.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Preserves COMMENT and HISTORY Cards**: These are metadata lines typically used to describe data provenance and processing history. The function captures them as lists of strings to preserve their order and meaning.\n",
    "  \n",
    "- **Handles Undefined or Problematic Values**: Some FITS header values may be of type `Undefined` (from `astropy.io.fits.card.Undefined`) or may not serialize well. These are either replaced with `None` or converted to strings.\n",
    "\n",
    "- **Skips Blank or Unusable Keywords**: FITS headers may include empty keywords or corrupted entries. The function skips these to avoid polluting the output.\n",
    "\n",
    "- **Ensures JSON Compatibility**: All values in the resulting dictionary are converted into basic Python types (e.g., `int`, `float`, `str`, `None`) that are fully compatible with JSON serialization or inclusion in other data formats like ASDF.\n",
    "\n",
    "This function is invoked during the ASDF file creation to ensure the full contents of the FITS header are embedded in a machine-readable, lossless format ‚Äî enabling future interpretability, transparency, and reproducibility of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df8cf79b-48e2-4bfe-bcdf-c4925b8505ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io.fits.card import Undefined\n",
    "\n",
    "def full_header_to_dict(header):\n",
    "    \"\"\"\n",
    "    Converts a FITS Header to a fully serializable dictionary,\n",
    "    including COMMENT and HISTORY cards as lists of strings.\n",
    "    Skips or replaces non-serializable values like Undefined.\n",
    "    \"\"\"\n",
    "    header_dict = {}\n",
    "\n",
    "    for card in header.cards:\n",
    "        key = card.keyword\n",
    "        val = card.value\n",
    "\n",
    "        # Skip blank keys\n",
    "        if key == \"\":\n",
    "            continue\n",
    "\n",
    "        # Handle COMMENT and HISTORY\n",
    "        if key == \"COMMENT\":\n",
    "            header_dict.setdefault(\"COMMENT\", []).append(str(val))\n",
    "        elif key == \"HISTORY\":\n",
    "            header_dict.setdefault(\"HISTORY\", []).append(str(val))\n",
    "        elif isinstance(val, Undefined):\n",
    "            header_dict[key] = None  # or skip with `continue`\n",
    "        else:\n",
    "            try:\n",
    "                header_dict[key] = val\n",
    "            except Exception:\n",
    "                header_dict[key] = str(val)\n",
    "\n",
    "    return header_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea021874-4322-44a5-bc07-8798ded958ba",
   "metadata": {},
   "source": [
    "## FITS File Processing and ASDF Construction\n",
    "\n",
    "The `process_fits_info` function is responsible for parsing raw FITS files and converting them into a structured, analysis-ready format. It plays a central role in preparing solar image data for downstream machine learning workflows.\n",
    "\n",
    "### Step-by-Step Function Overview\n",
    "\n",
    "- **Filename Parsing & FITS Loading**: Each FITS file is opened and verified using `astropy.io.fits`. The function automatically selects the appropriate HDU for processing (either primary or secondary based on availability of image data).\n",
    "\n",
    "- **Timestamp Extraction**: The observation timestamp is extracted from the FITS header using `T_OBS`, `DATE-OBS`, or `TIME-OBS` keywords. This timestamp is critical for time-based querying of solar event data.\n",
    "\n",
    "- **Storage Path Construction**: A unique path is constructed using the instrument name, wavelength, and timestamp, which is used to logically organize data and generate S3-style URLs for metadata purposes (even though everything is saved locally).\n",
    "\n",
    "- **Header & Data Conversion**:\n",
    "  - The FITS header is converted to both `.json` and `.txt` formats.\n",
    "  - The image data is saved in `.csv` and `.npy` formats for tabular and binary use cases.\n",
    "  - All outputs are saved under `final_fits_1/fits/{header|data}`.\n",
    "\n",
    "- **Image Generation**:\n",
    "  - A log-scaled, grayscale-stretched version of the FITS image is generated and saved as `.png` and `.jp2`.\n",
    "  - This aids both visualization and ML preprocessing.\n",
    "\n",
    "- **Event Metadata Extraction**:\n",
    "  - HEK (Heliophysics Event Knowledgebase) data is retrieved based on the observation time and saved as JSON files.\n",
    "  - DONKI data retrieval is commented out for this study but can be enabled for future analyses.\n",
    "\n",
    "- **ASDF File Creation**:\n",
    "  - The processed FITS data, HEK event metadata, image paths, and pre-assigned image labels are passed to the `build_and_save_asdf` function to generate a complete, portable `.asdf` archive.\n",
    "  - These ASDF files include all critical solar observation and annotation information for each image, suitable for model training or further scientific analysis.\n",
    "\n",
    "- **Local Storage**: All outputs‚Äîincluding headers, data, images, HEK metadata, and ASDF files‚Äîare saved locally to directories under `final_fits_1/`, without uploading to S3 during this stage.\n",
    "\n",
    "This function is a core utility for transforming observational solar physics data into a unified, structured, and AI/ML-ready format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0477d433-a3b8-4e0e-a6a8-de6b45c3d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fits_info(rootPath, filePath, bucket, spacecraft, instrument, wavelength, detector):\n",
    "    for file_path in filePath:\n",
    "        fname = os.path.basename(file_path).replace(\".fits\", \"\")\n",
    "        print(f\"\\n Processing: {fname}\")\n",
    "        hdul = fits.open(file_path)\n",
    "        hdul.verify('fix')\n",
    "        hdu = hdul[1] if len(hdul) > 1 and hdul[1].data is not None else hdul[0]\n",
    "\n",
    "        data = hdu.data\n",
    "        header = hdu.header\n",
    "\n",
    "        # Get observed datetime\n",
    "        if \"T_OBS\" in header:\n",
    "            obs_time = header[\"T_OBS\"].rstrip(\"Z\")\n",
    "        elif \"DATE-OBS\" in header and \"T\" in header[\"DATE-OBS\"]:\n",
    "            obs_time = header[\"DATE-OBS\"]\n",
    "        elif \"DATE-OBS\" in header and \"TIME-OBS\" in header:\n",
    "            obs_time = header[\"DATE-OBS\"] + \"T\" + header[\"TIME-OBS\"]\n",
    "        else:\n",
    "            obs_time = None\n",
    "\n",
    "        dt_obs = dtparser.parse(obs_time)\n",
    "\n",
    "        # Build directory structure\n",
    "        if spacecraft == \"SDO\":\n",
    "            base_key = f\"{instrument}/{wavelength}/{dt_obs.strftime('%Y/%m/%d/%H/%M')}\"\n",
    "        else:  # SOHO\n",
    "            base_key = f\"{instrument}/{detector}/{dt_obs.strftime('%Y/%m/%d/%H/%M')}\"\n",
    "\n",
    "        # Header/Data Conversion\n",
    "        header_dict = {\n",
    "            card.keyword: card.value\n",
    "            for card in header.cards\n",
    "            if card.keyword and not isinstance(card.value, Undefined)\n",
    "        }\n",
    "\n",
    "        formats = [\n",
    "            (\"header\", \"fits/header\", [\n",
    "                (\"json\", lambda: json.dumps(header_dict, indent=2)),\n",
    "                (\"txt\", lambda: \"\\n\".join([f\"{k} = {v}\" for k, v in header_dict.items()]))\n",
    "            ]),\n",
    "            (\"data\", \"fits/data\", [\n",
    "                (\"csv\", lambda: pd.DataFrame(data).to_csv(index=False)),\n",
    "                (\"npy\", lambda: data)\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "\n",
    "        for category, folder, files in formats:\n",
    "            # local_out_dir = os.path.join(\"final_fits_1\", folder)\n",
    "            local_out_dir = os.path.join(EXTERNAL_DRIVE_BASE, folder)\n",
    "            os.makedirs(local_out_dir, exist_ok=True)\n",
    "\n",
    "            for ext, content_fn in files:\n",
    "                local_out_path = os.path.join(local_out_dir, f\"{fname}_{category}.{ext}\")\n",
    "\n",
    "                if ext == \"npy\":\n",
    "                    np.save(local_out_path, content_fn())\n",
    "                else:\n",
    "                    with open(local_out_path, \"w\") as f:\n",
    "                        f.write(content_fn())\n",
    "\n",
    "                print(f\"Saved locally to {local_out_path}\")\n",
    "\n",
    "        # Image (log stretch)\n",
    "        data_clipped = np.clip(data, a_min=1e-3, a_max=None)\n",
    "        data_log = np.log10(data_clipped)\n",
    "        data_norm = 255 * (data_log - np.nanmin(data_log)) / (np.nanmax(data_log) - np.nanmin(data_log))\n",
    "        data_uint8 = np.nan_to_num(data_norm).astype(np.uint8)\n",
    "\n",
    "        # img_out_dir = os.path.join(\"final_fits_1\", \"images\")\n",
    "        \n",
    "        img_out_dir = os.path.join(EXTERNAL_DRIVE_BASE, \"images\")\n",
    "        os.makedirs(img_out_dir, exist_ok=True)\n",
    "\n",
    "        for kind in [\"log_stretch\"]:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(data_log, cmap='gray', origin='lower')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # for ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "            for ext in [\"png\"]:\n",
    "                img_path = os.path.join(img_out_dir, f\"{fname}_{kind}.{ext}\")\n",
    "                plt.savefig(img_path, dpi=300)\n",
    "                print(f\"Saved image: {img_path}\")\n",
    "\n",
    "            # Save JP2 separately\n",
    "            jp2_path = os.path.join(img_out_dir, f\"{fname}_{kind}.jp2\")\n",
    "            img = Image.fromarray(data_uint8)\n",
    "            img.save(jp2_path, format=\"JPEG2000\")\n",
    "            print(f\"Saved JP2 image: {jp2_path}\")\n",
    "\n",
    "            plt.close()\n",
    "        \n",
    "        # Fetch and dump raw HEK and DONKI data\n",
    "        # hek_json_path = os.path.join(\"final_fits_1\", \"hek\", f\"{fname}_hek.json\")\n",
    "        hek_json_path = os.path.join(EXTERNAL_DRIVE_BASE, \"hek\", f\"{fname}_hek.json\")\n",
    "        \n",
    "        # donki_json_path = os.path.join(\"final_fits_1\", \"donki\", f\"{fname}_donki.json\")\n",
    "        os.makedirs(os.path.dirname(hek_json_path), exist_ok=True)\n",
    "        # os.makedirs(os.path.dirname(donki_json_path), exist_ok=True)\n",
    "\n",
    "        hek_data = fetch_hek_events(dt_obs, hek_json_path)\n",
    "        # donki_data = fetch_donki_events(dt_obs, donki_json_path)\n",
    "\n",
    "        if hek_data:\n",
    "            print(f\"HEK data saved to {hek_json_path} for {dt_obs}\")\n",
    "\n",
    "        image_paths = [\n",
    "            f\"https://{bucket}.s3.us-east-1.amazonaws.com/{base_key}/images/{fname}_log_stretch.{ext}\"\n",
    "            for ext in [\"png\", \"jp2\"]\n",
    "        ]\n",
    "\n",
    "        build_and_save_asdf(\n",
    "            fname=fname,\n",
    "            hdul=hdul,\n",
    "            hek_data=hek_data,\n",
    "            donki_data=None,\n",
    "            image_paths=image_paths,\n",
    "            bucket=bucket,\n",
    "            base_key=base_key,\n",
    "            rootPath=rootPath\n",
    "        )\n",
    "\n",
    "        hdul.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203bd812-1157-4053-94cd-c0d7f9f9cb06",
   "metadata": {},
   "source": [
    "## Batch Processing of Local FITS Files\n",
    "\n",
    "The `process_local_fits_directory` function serves as the main entry point for initiating batch processing of all FITS files stored locally in a specified directory. It prepares the necessary inputs and passes them to the `process_fits_info` function for full pipeline execution.\n",
    "\n",
    "### Function Purpose\n",
    "\n",
    "This function scans a local directory for `.fits` files and orchestrates the processing of each file through the FITS-to-ASDF pipeline. While the function includes references to S3-compatible paths (to simulate cloud-based organization), all operations are performed and stored locally.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Directory Scanning**: It automatically detects and lists all `.fits` files in the specified directory.\n",
    "- **Validation**: If no FITS files are found, the function halts and prints a warning.\n",
    "- **Dynamic Configuration**:\n",
    "  - Chooses the appropriate mock S3 bucket name (`helioconvert-sdo` or `helioconvert-soho`) based on the selected spacecraft.\n",
    "  - Accepts instrument, wavelength, and optional detector information to construct the appropriate processing pipeline.\n",
    "- **Pipeline Trigger**: Once the file list is prepared, it calls the `process_fits_info` function, which performs:\n",
    "  - Header and data extraction\n",
    "  - Image generation\n",
    "  - HEK event querying\n",
    "  - ASDF archive creation\n",
    "\n",
    "### Note\n",
    "\n",
    "No actual uploading to Amazon S3 occurs in this stage. Bucket names are used to maintain consistency in file naming, organization, and potential future migration to cloud storage.\n",
    "\n",
    "This function allows for streamlined, large-scale data ingestion and transformation for any directory of pre-renamed FITS files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840db597-4508-42a6-baca-900d4f26b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_local_fits_directory(local_dir, spacecraft, instrument, wavelength=None, detector=None):\n",
    "    \"\"\"\n",
    "    Process all renamed FITS files in a local directory and upload to S3.\n",
    "    \"\"\"\n",
    "    bucket = \"helioconvert-sdo\" if spacecraft.upper() == \"SDO\" else \"helioconvert-soho\"\n",
    "\n",
    "    # üîç Grab all FITS files\n",
    "    file_list = [os.path.join(local_dir, f) for f in os.listdir(local_dir) if f.endswith(\".fits\")]\n",
    "    \n",
    "    if not file_list:\n",
    "        print(\"‚ùå No FITS files found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Found {len(file_list)} FITS files in '{local_dir}'.\")\n",
    "\n",
    "    process_fits_info(local_dir, file_list, bucket, spacecraft, instrument, wavelength, detector)\n",
    "\n",
    "\n",
    "process_local_fits_directory(\n",
    "    local_dir=\"data_selection/fits/raw\",\n",
    "    spacecraft=\"SDO\",\n",
    "    instrument=\"aia\",\n",
    "    wavelength=\"131\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
