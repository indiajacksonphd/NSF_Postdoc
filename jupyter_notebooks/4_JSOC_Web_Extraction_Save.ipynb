{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69f4497-6dfc-471a-a7d0-35be6980451a",
   "metadata": {},
   "source": [
    "### JSOC Data Export: Why I Used the Web App Instead of SunPy/VSO\n",
    "\n",
    "The **JSOC Data Export web application** is an online tool hosted by Stanford University that allows users to retrieve high-quality, calibrated solar data products‚Äîsuch as AIA Level 1 EUV images‚Äîfrom the Solar Dynamics Observatory (SDO).\n",
    "\n",
    "While tools like **SunPy** and **VSO (Virtual Solar Observatory)** offer Python APIs to query and download SDO data, they often suffer from limitations such as:\n",
    "- **Unreliable API performance** (timeouts, failed connections, or partial downloads)\n",
    "- **Limited bulk retrieval support** for large date ranges\n",
    "- **Rate limiting** or internal server issues during peak times\n",
    "\n",
    "#### Why I Chose the JSOC Web App\n",
    "Sometimes, it is simply **more efficient and reliable** to use the JSOC Export tool manually‚Äîespecially when:\n",
    "- Retrieving data for well-defined time intervals (e.g., around solar flare events)\n",
    "- Avoiding timeout errors or broken download links\n",
    "- Ensuring 100% data availability without worrying about batch errors\n",
    "\n",
    "#### Manual Workflow Overview\n",
    "To guarantee clean, complete downloads for each flare event, I followed these manual steps:\n",
    "\n",
    "1. **Go to the JSOC Data Export web app**:  \n",
    "   https://jsoc.stanford.edu/ajax/exportdata.html\n",
    "\n",
    "2. **Fill in the input boxes**:  \n",
    "   Enter the recordset string for the flare window, such as:  \n",
    "   ```\n",
    "   aia.lev1_euv_12s[2014-04-18T12:45:00/180m][131]\n",
    "   ```\n",
    "\n",
    "3. **Submit and wait for email**:  \n",
    "   The app sends a unique export link to your email.\n",
    "\n",
    "4. **Open the emailed link**:  \n",
    "   This link leads to a temporary webpage listing all `.fits` files available for download.\n",
    "\n",
    "5. **View page source**:  \n",
    "   Right-click ‚Üí ‚ÄúView Page Source‚Äù to reveal the raw HTML.\n",
    "\n",
    "6. **Copy the entire HTML code**:  \n",
    "   Save it into a local `.html` file for parsing.\n",
    "\n",
    "7. **Parse and download**:  \n",
    "   Use `BeautifulSoup` and Python to extract all non-\"spikes\" `.fits` links and download them using `requests`.\n",
    "\n",
    "This method ensures **full control, reliability, and transparency**‚Äîkey when working with high-volume solar datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b1f91-836e-4db6-a7e6-026ea8804a9c",
   "metadata": {},
   "source": [
    "### üîó Click the image below to open the JSOC Data Export Tool\n",
    "\n",
    "[![Open JSOC Data Export Tool](https://helioconverter-web-application.s3.us-east-1.amazonaws.com/JSOC_Homepage.png)](http://jsoc.stanford.edu/ajax/exportdata.html?ds=aia.lev1_euv_12s[2020-01-01T00:00:00/20m][131])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541a726-0519-462b-ab80-fb8728c75876",
   "metadata": {},
   "source": [
    "### What This Function Does ‚Äî Step-by-Step\n",
    "\n",
    "1. **Load Event Data**  \n",
    "   Reads a CSV file containing solar flare events, including their start, peak, and end times, as well as buffer times used for downloading data.\n",
    "\n",
    "2. **Iterate Through Events**  \n",
    "   For each row (flare event) in the CSV:\n",
    "   - Extracts timestamps for the start/end of the flare and the buffered observation window.\n",
    "   - Converts the flare date into a folder name for organizing downloaded files.\n",
    "   - Skips over any flare dates that are missing or intentionally excluded (e.g., January 7, 2014).\n",
    "\n",
    "3. **Parse Corresponding HTML Dump**  \n",
    "   For each flare date, the function opens a pre-downloaded HTML file (from JSOC) containing links to FITS files. It uses this file to extract only the links to valid `.fits` files, excluding any that contain ‚Äúspikes.‚Äù\n",
    "\n",
    "4. **Download FITS Files**  \n",
    "   The extracted links are downloaded into the corresponding flare date folder.  \n",
    "   - Skips any files that already exist.  \n",
    "   - Skips files that are too small to be valid.  \n",
    "   - Progress is shown using a status bar.\n",
    "\n",
    "5. **Sort Files by Time Phase**  \n",
    "   Once downloaded, the files are grouped into three categories based on timestamps:\n",
    "   - **Pre-flare**: From buffer start to flare start  \n",
    "   - **Flare**: From flare start to flare end  \n",
    "   - **Post-flare**: From flare end to buffer end\n",
    "\n",
    "6. **Move Files into Subfolders**  \n",
    "   Each group of files is moved into its corresponding subfolder (`pre`, `flare`, or `post`) within the flare‚Äôs main directory.\n",
    "\n",
    "7. **Clean Up**  \n",
    "   Any leftover `.fits` files still sitting in the parent folder (not sorted) are deleted, leaving only the three clean subdirectories.\n",
    "\n",
    "8. **Repeat for All Events**  \n",
    "   This process continues for every event listed in the CSV until all have been processed, downloaded, and neatly organized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdbde6-1787-45d1-9cce-d02a33f32240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_and_organize_fits(csv_path, html_dir, fits_root, wavelength=\"131\"):\n",
    "    def filename_from_timestamp(ts):\n",
    "        return f\"aia.lev1_euv_12s.{ts.strftime('%Y-%m-%dT%H%M%S')}Z.{wavelength}.image_lev1.fits\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            jsoc_start = datetime.strptime(row[\"jsoc_start_time\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "            flare_start = datetime.strptime(row[\"event_starttime\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "            flare_end = datetime.strptime(row[\"event_endtime\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "            jsoc_end = datetime.strptime(row[\"jsoc_end_time\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "        except Exception as e:\n",
    "            print(f\"Row {idx} skipped (timestamp error): {e}\")\n",
    "            continue\n",
    "\n",
    "        folder_name = flare_start.strftime(\"%Y-%m-%d\")\n",
    "        # if folder_name == \"2014-01-07\":\n",
    "            # continue\n",
    "\n",
    "        html_file_path = os.path.join(html_dir, f\"{folder_name}.html\")\n",
    "        if not os.path.exists(html_file_path):\n",
    "            print(f\"HTML file not found for {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(fits_root, folder_name)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Step 1: Parse HTML and download links\n",
    "        with open(html_file_path, \"r\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "        links = [\n",
    "            a['href'] for a in soup.find_all('a', href=True)\n",
    "            if a['href'].endswith('.fits') and 'spikes' not in a['href']\n",
    "        ]\n",
    "        full_urls = [\n",
    "            link if link.startswith(\"http\") else f\"https://jsoc1.stanford.edu{link}\"\n",
    "            for link in links\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n Downloading {len(full_urls)} files for {folder_name}...\")\n",
    "\n",
    "        for url in tqdm(full_urls, desc=f\"üì• {folder_name}\"):\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                continue\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=60)\n",
    "                response.raise_for_status()\n",
    "                content_length = int(response.headers.get(\"Content-Length\", 0))\n",
    "                if content_length < 10_000:\n",
    "                    print(f\"Skipped small file: {filename} ({content_length} bytes)\")\n",
    "                    continue\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "        # Step 2: Partition into pre, flare, post\n",
    "        all_files = sorted(f for f in os.listdir(folder_path) if f.endswith(\".fits\"))\n",
    "        jsoc_start_file = filename_from_timestamp(jsoc_start)\n",
    "        flare_start_file = filename_from_timestamp(flare_start)\n",
    "        flare_end_file = filename_from_timestamp(flare_end)\n",
    "        jsoc_end_file = filename_from_timestamp(jsoc_end)\n",
    "\n",
    "        pre_files = [f for f in all_files if jsoc_start_file <= f < flare_start_file]\n",
    "        flare_files = [f for f in all_files if flare_start_file <= f <= flare_end_file]\n",
    "        post_files = [f for f in all_files if flare_end_file < f <= jsoc_end_file]\n",
    "\n",
    "        for phase, file_list in [(\"pre\", pre_files), (\"flare\", flare_files), (\"post\", post_files)]:\n",
    "            target_dir = os.path.join(folder_path, phase)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            for fname in file_list:\n",
    "                src = os.path.join(folder_path, fname)\n",
    "                dst = os.path.join(target_dir, fname)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.move(src, dst)\n",
    "\n",
    "        # Step 3: Delete any .fits files left in the parent folder\n",
    "        leftovers = [f for f in os.listdir(folder_path) if f.endswith(\".fits\")]\n",
    "        for fname in leftovers:\n",
    "            try:\n",
    "                os.remove(os.path.join(folder_path, fname))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete leftover file {fname}: {e}\")\n",
    "\n",
    "        print(f\"Organized {folder_name}: {len(pre_files)} pre, {len(flare_files)} flare, {len(post_files)} post\")\n",
    "\n",
    "    print(\"\\nAll events processed and sorted.\")\n",
    "\n",
    "    \n",
    "download_and_organize_fits(\n",
    "    csv_path=\"flare_summary_final/flare_selection/strongest_flares_2014_SDO_AIA_131.csv\",\n",
    "    html_dir=\"html_dump\",\n",
    "    fits_root=\"final_fits\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a587c94-7d10-42c5-b0a1-0388f7fca2c3",
   "metadata": {},
   "source": [
    "### Flare Duration Calculation (Discrete Minute Binning)\n",
    "\n",
    "In this analysis, we define the duration of each solar flare event using **discrete minute-level bins**, rather than continuous time intervals. This approach aligns with how FITS image files are timestamped ‚Äî down to the minute and second (e.g., `T081811Z`), with a consistent cadence of one image approximately every 12 seconds.\n",
    "\n",
    "Rather than using floating-point duration (e.g., 103.4 minutes), we:\n",
    "\n",
    "1. **Count the number of unique `hhmm` values** between the flare's official start and end timestamps.\n",
    "2. **Include both the starting and ending minutes** (i.e., inclusive binning).\n",
    "3. Use the resulting integer count to represent the number of expected image rows during the flare.\n",
    "\n",
    "This is statistically equivalent to **rounding up (ceiling)** the true flare duration to ensure complete coverage of the flare window. The discrete duration better reflects how the data are stored and accessed, and ensures consistency in file-based image selection workflows.\n",
    "\n",
    "We also log both:\n",
    "- The **continuous duration** in minutes (for reference).\n",
    "- The **discretized count of unique minutes** (used for all sampling and model input).\n",
    "\n",
    "Each flare's total image count is then calculated as:\n",
    "\n",
    "```text\n",
    "Total Images = Pre-Flare (178) + Flare (discrete) + Post-Flare (178)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6e8c1-2146-45af-8891-64f19e18c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"flare_summary_final/flare_selection/strongest_flares_2014_SDO_AIA_131.csv\", parse_dates=[\n",
    "    'event_starttime', 'event_endtime', 'jsoc_start_time', 'jsoc_end_time'])\n",
    "\n",
    "# Continuous durations (still used for display mapping)\n",
    "df['flare_duration_sec'] = (df['event_endtime'] - df['event_starttime']).dt.total_seconds()\n",
    "df['flare_duration_min'] = df['flare_duration_sec'] / 60\n",
    "\n",
    "# Discrete flare minute counter using unique hhmm values\n",
    "def count_unique_minutes(start, end):\n",
    "    times = pd.date_range(start=start.floor('min'), end=end.floor('min'), freq='T')\n",
    "    unique_hhmms = {t.strftime(\"%H%M\") for t in times}\n",
    "    return len(unique_hhmms), unique_hhmms\n",
    "\n",
    "# Apply function to get discrete durations\n",
    "df['flare_duration_discrete'], df['flare_hhmm_bins'] = zip(*df.apply(\n",
    "    lambda row: count_unique_minutes(row['event_starttime'], row['event_endtime']),\n",
    "    axis=1\n",
    "))\n",
    "df['flare_duration_min_discrete'] = df['flare_duration_discrete']\n",
    "\n",
    "# Add pre-flare and post-flare counts\n",
    "df['pre_flare'] = 178\n",
    "df['post_flare'] = 178\n",
    "df['total_per_event'] = df['flare_duration_min_discrete'] + df['pre_flare'] + df['post_flare']\n",
    "\n",
    "# Format for display: show float minutes ‚Üí discrete count\n",
    "df['flare_display'] = df.apply(\n",
    "    lambda row: f\"{row['flare_duration_min']:.1f} ‚Üí {row['flare_duration_min_discrete']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create display table\n",
    "summary_df = df[['event_starttime', 'flare_display', 'pre_flare', 'post_flare', 'total_per_event']].copy()\n",
    "summary_df.columns = ['Event Date', 'Flare Images', 'Pre-Flare (0)', 'Post-Flare (0)', 'Total per Event']\n",
    "summary_df['Event Date'] = summary_df['Event Date'].dt.date\n",
    "\n",
    "# Add separator row\n",
    "separator_row = pd.DataFrame([['‚Äî'] * len(summary_df.columns)], columns=summary_df.columns)\n",
    "\n",
    "# Add TOTAL row using discrete durations\n",
    "total_row = pd.DataFrame([[\n",
    "    'TOTAL',\n",
    "    df['flare_duration_min_discrete'].sum(),\n",
    "    df['pre_flare'].sum(),\n",
    "    df['post_flare'].sum(),\n",
    "    df['total_per_event'].sum()\n",
    "]], columns=summary_df.columns)\n",
    "\n",
    "# Concatenate and print\n",
    "summary_df = pd.concat([summary_df, separator_row, total_row], ignore_index=True)\n",
    "\n",
    "print(\"Final Summary Per Event\\n\")\n",
    "print(tabulate(summary_df, headers='keys', tablefmt='github', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6dd97-1d4d-4cb9-b928-5e9f56560c8f",
   "metadata": {},
   "source": [
    "### Stratified Random Sampling of Minute-Level FITS Files\n",
    "\n",
    "To prepare a diverse and balanced training dataset for flare classification, we implemented a **stratified random sampling strategy** based on the `hhmm` timestamp of each FITS file.\n",
    "\n",
    "#### Process Overview:\n",
    "- Each flare folder contains dozens to hundreds of FITS files with timestamps in the format:  \n",
    "  `aia.lev1_euv_12s.2014-02-20T081811Z.131.image_lev1.fits`\n",
    "- The script extracts the `hhmm` (hour‚Äìminute) portion from each filename.\n",
    "- We count how many files occur in each `hhmm` group using a `Counter`.\n",
    "- For every unique `hhmm`, we:\n",
    "  - Track the index position(s) of all files within that minute.\n",
    "  - **Randomly select one FITS file** to represent that minute bin.\n",
    "- The selected FITS file is copied into a central folder called `data_selection`.\n",
    "\n",
    "This process guarantees:\n",
    "- One file per minute (if multiple are present).\n",
    "- Balanced temporal representation across the flare duration.\n",
    "- True randomness **within each bin**, ensuring reproducibility.\n",
    "\n",
    "#### Sampling Logs:\n",
    "- For transparency and reproducibility, we save a `.csv` log for each flare in the `sampling_logs` directory.\n",
    "- Each log includes:\n",
    "  - `hhmm` group\n",
    "  - Number of files in the group (`count`)\n",
    "  - Index positions of files in the group (`place`)\n",
    "  - Which file was selected (`chosen_file` and `chosen_index`)\n",
    "\n",
    "> **Example Output Log:**\n",
    "> ```\n",
    "> hhmm,count,place,chosen_index,chosen_file  \n",
    "> 0723,1,[0],0,aia.lev1...  \n",
    "> 0724,5,[1,2,3,4,5],3,aia.lev1...  \n",
    "> 0725,5,[6,7,8,9,10],8,aia.lev1...\n",
    "> ```\n",
    "\n",
    "#### Image Labeling:\n",
    "- All copied files are labeled as either `1` (flare) or `0` (non-flare: pre/post) and saved in a master label file.\n",
    "- The labels are stored under the same filenames in both `image_labels.csv` (comma-separated) and `image_labels.txt` (tab-separated), located in the `data_selection` folder for direct use in ML workflows.\n",
    "\n",
    "> **Example Format (CSV or TXT):**  \n",
    "> ```\n",
    "> filename,label\n",
    "> aia.lev1_euv_12s.2014-02-20T081811Z.131.image_lev1.fits,1  \n",
    "> aia.lev1_euv_12s.2014-02-20T074011Z.131.image_lev1.fits,0  \n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900099f-b41c-4a45-bf3d-eaf661e7457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Folders to process\n",
    "flare_folders = [\n",
    "    \"/Users/indiajackson/PycharmProjects/1_NSF_Postdoc/FITSvsASDF/JSOC_Events/final_fits/2014-01-07\",\n",
    "    \"/Users/indiajackson/PycharmProjects/1_NSF_Postdoc/FITSvsASDF/JSOC_Events/final_fits/2014-02-20\",\n",
    "    \"/Users/indiajackson/PycharmProjects/1_NSF_Postdoc/FITSvsASDF/JSOC_Events/final_fits/2014-02-25\",\n",
    "    \"/Users/indiajackson/PycharmProjects/1_NSF_Postdoc/FITSvsASDF/JSOC_Events/final_fits/2014-04-18\",\n",
    "    \"/Users/indiajackson/PycharmProjects/1_NSF_Postdoc/FITSvsASDF/JSOC_Events/final_fits/2014-09-10\"\n",
    "]\n",
    "\n",
    "data_selection = \"data_selection/fits/raw\"\n",
    "data_selection_parent = \"data_selection\"\n",
    "\n",
    "log_dir = \"data_selection/sampling_logs\"\n",
    "os.makedirs(data_selection, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "file_labels = []\n",
    "\n",
    "for base_dir in flare_folders:\n",
    "    flare_dir = os.path.join(base_dir, \"flare\")\n",
    "    pre_dir = os.path.join(base_dir, \"pre\")\n",
    "    post_dir = os.path.join(base_dir, \"post\")\n",
    "\n",
    "    print(f\"\\n Processing: {flare_dir}\")\n",
    "    fits_files = sorted([f for f in os.listdir(flare_dir) if f.endswith(\".fits\")])\n",
    "    hhmm_counter = Counter()\n",
    "\n",
    "    # Count by hhmm\n",
    "    for fname in fits_files:\n",
    "        try:\n",
    "            timestamp = fname.split(\"T\")[1]\n",
    "            hhmm = timestamp[:4]\n",
    "            hhmm_counter[hhmm] += 1\n",
    "        except IndexError:\n",
    "            print(f\" Skipping malformed filename: {fname}\")\n",
    "\n",
    "    # Stratified random selection for flare (label = 1)\n",
    "    sorted_items = sorted(hhmm_counter.items())\n",
    "    place_counter = 0\n",
    "    rows = []\n",
    "\n",
    "    for hhmm, count in sorted_items:\n",
    "        place_list = list(range(place_counter, place_counter + count))\n",
    "        chosen_index = random.choice(place_list)\n",
    "        chosen_file = fits_files[chosen_index]\n",
    "\n",
    "        src = os.path.join(flare_dir, chosen_file)\n",
    "        dst = os.path.join(data_selection, chosen_file)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            file_labels.append((chosen_file, 1))\n",
    "\n",
    "        rows.append({\n",
    "            \"hhmm\": hhmm,\n",
    "            \"count\": count,\n",
    "            \"place\": place_list,\n",
    "            \"chosen_index\": chosen_index,\n",
    "            \"chosen_file\": chosen_file\n",
    "        })\n",
    "\n",
    "        place_counter += count\n",
    "\n",
    "    # Save stratified flare sample log\n",
    "    flare_date = base_dir.split(\"/\")[-1]\n",
    "    csv_name = f\"{flare_date}.csv\"\n",
    "    csv_path = os.path.join(log_dir, csv_name)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved flare sampling log to `{csv_path}`\")\n",
    "    print(f\"Copied {len(rows)} flare FITS files to `{data_selection}`\")\n",
    "\n",
    "    # Validation\n",
    "    total_from_counter = sum(hhmm_counter.values())\n",
    "    total_in_directory = len(fits_files)\n",
    "    print(f\"FITS in dir: {total_in_directory} | Counted: {total_from_counter}\")\n",
    "    if total_from_counter == total_in_directory:\n",
    "        print(\"Count matches perfectly.\")\n",
    "    else:\n",
    "        print(\"Mismatch detected!\")\n",
    "\n",
    "    # Copy 178 pre/post-flare files (label = 0)\n",
    "    for tag, path in zip(['pre', 'post'], [pre_dir, post_dir]):\n",
    "        tag_files = [f for f in os.listdir(path) if f.endswith(\".fits\")]\n",
    "        selected = tag_files if len(tag_files) <= 178 else random.sample(tag_files, 178)\n",
    "        print(f\"Copying {len(selected)} {tag}-flare files...\")\n",
    "\n",
    "        for f in selected:\n",
    "            src = os.path.join(path, f)\n",
    "            dst = os.path.join(data_selection, f)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "                file_labels.append((f, 0))\n",
    "\n",
    "# Save full label list\n",
    "label_df = pd.DataFrame(file_labels, columns=[\"filename\", \"label\"])\n",
    "label_df = label_df.sort_values(\"filename\")  # üîç Sort alphabetically\n",
    "# label_df.to_csv(\"data_selection_labels.csv\", index=False)\n",
    "label_df.to_csv(os.path.join(data_selection_parent, \"image_labels.csv\"), index=False)\n",
    "label_df.to_csv(os.path.join(data_selection_parent, \"image_labels.txt\"), index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"\\nSaved `image_labels.csv` and `image_labels.txt` to data_selection/\")\n",
    "print(\"\\nSaved full label file to `image_labels.csv`\")\n",
    "print(f\"Data extraction complete. Total files: {len(file_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
